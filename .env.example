# =============================================================================
# arXiv AI - Environment Configuration
# =============================================================================
# Copy this file to .env and update with your actual values
# cp .env.example .env

# =============================================================================
# Database Configuration
# =============================================================================
# Option 1: Use DATABASE_URL (recommended)
DATABASE_URL=mysql://user:password@localhost:3306/arxiv_db

# Option 2: Use individual settings (alternative to DATABASE_URL)
# DB_HOST=localhost
# DB_PORT=3306
# DB_NAME=arxiv_db
# DB_USER=your_user
# DB_PASSWORD=your_password

# =============================================================================
# arXiv Configuration
# =============================================================================
# Comma-separated list of arXiv categories to search
ARXIV_CATEGORIES=cs.AI,cs.LG,cs.CV,cs.CL,cs.NE

# Rate limiting (seconds between API calls)
RATE_LIMIT_SECONDS=4

# Maximum results per search query
MAX_RESULTS_PER_SEARCH=10

# =============================================================================
# Embedding Configuration
# =============================================================================
# Embedding provider: "sentence-transformers" or "openai"
EMBEDDING_PROVIDER=sentence-transformers

# Embedding model name
# For sentence-transformers: any HuggingFace model (e.g., all-MiniLM-L6-v2)
# For OpenAI: text-embedding-3-small, text-embedding-3-large, text-embedding-ada-002
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# OpenAI Configuration (required if using OpenAI provider)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# =============================================================================
# Qdrant Vector Database Configuration
# =============================================================================
# Local Qdrant (default)
QDRANT_HOST=localhost
QDRANT_PORT=6333

# Cloud Qdrant (alternative to HOST/PORT)
# QDRANT_URL=https://your-cluster.qdrant.io
# QDRANT_API_KEY=your_qdrant_api_key

# Collection name for storing paper chunks
QDRANT_COLLECTION_NAME=paper_chunks

# =============================================================================
# Chunking Configuration
# =============================================================================
# Target chunk size in tokens (500-800 recommended)
CHUNK_SIZE_TOKENS=650

# Overlap between chunks in tokens (100-150 recommended)
CHUNK_OVERLAP_TOKENS=125

# =============================================================================
# Retrieval API Configuration
# =============================================================================
# API server host
API_HOST=0.0.0.0

# API server port
API_PORT=8000

# Enable debug mode (true/false)
DEBUG=false

# CORS origins (comma-separated)
CORS_ORIGINS=http://localhost:3000,http://localhost:8080

# Search configuration
DEFAULT_TOP_K=10
MAX_TOP_K=100

# =============================================================================
# LLM Configuration (for RAG)
# =============================================================================
# LLM provider: "openai", "ollama", "huggingface", "lmstudio", "anthropic"
LLM_PROVIDER=openai

# LLM model name (provider-specific)
# OpenAI: "gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"
# Ollama: "llama2", "mistral", "codellama", "phi"
# HuggingFace: "meta-llama/Llama-2-7b-chat-hf" (any HuggingFace model ID)
# LM Studio: Model name as configured in LM Studio
# Anthropic: "claude-3-opus", "claude-3-sonnet", "claude-3-haiku"
LLM_MODEL=gpt-4

# Base URL for local providers (Ollama, LM Studio)
# Ollama default: http://localhost:11434
# LM Studio default: http://localhost:1234/v1
# LLM_BASE_URL=http://localhost:11434

# Generic API key (falls back to provider-specific keys)
# LLM_API_KEY=your_api_key_here

# LLM generation parameters
LLM_MAX_TOKENS=2000
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=120

# Device for HuggingFace models: "auto", "cpu", "cuda"
LLM_DEVICE=auto

# Anthropic API key (required if using Anthropic provider)
# ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# RAG Configuration
# =============================================================================
# Enable query simplification (breaks down complex queries)
QUERY_SIMPLIFICATION_ENABLED=true

# Maximum number of chunks to include in context
MAX_CHUNKS_FOR_CONTEXT=10
